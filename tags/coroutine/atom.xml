<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>coroutine on wd and cc</title>
    <link>https://wdicc.com/tags/coroutine/</link>
    <description>Recent content in coroutine on wd and cc</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 17 Oct 2019 10:49:40 +0800</lastBuildDate>
    
	<atom:link href="https://wdicc.com/tags/coroutine/atom.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Learning Python Coroutine</title>
      <link>https://wdicc.com/learning-python-coroutine/</link>
      <pubDate>Thu, 17 Oct 2019 10:49:40 +0800</pubDate>
      
      <guid>https://wdicc.com/learning-python-coroutine/</guid>
      <description>看了一个视频 OSB 2015 - How Do Python Coroutines Work?，从头开始讲 coroutine 是怎么抽象出来的，感觉好厉害。自己写了一点程序学习了一下。之前写的关于 coroutine 的帖子。  先准备一个 server.py ，可以接受客户端请求。要注意的是要使用 Threading ，或者 fork 的 server，要不服务端执行并不支持并发，需要处理完一个才能处理下一个，这样会发现虽然客户端那边请求是并发的，但是返回结果的时候是顺序的。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import socketserver from time import sleep HOST = &amp;#39;127.0.0.1&amp;#39; PORT = 2045 class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024) sleep(1) self.request.sendall(&amp;#39;{} ok&amp;#39;.format(data.decode()).encode()) def run(): server = socketserver.ThreadingTCPServer((HOST, PORT), MyTCPHandler) server.</description>
    </item>
    
    <item>
      <title>Python Coroutine</title>
      <link>https://wdicc.com/python-coroutine/</link>
      <pubDate>Tue, 27 Aug 2019 10:49:51 +0800</pubDate>
      
      <guid>https://wdicc.com/python-coroutine/</guid>
      <description>协程 coroutine 不知道是从什么时候开始的，感觉我第一次看到是 lua 里面支持 yield 。后面看到就是 javascript 里面的 Promise，async 和 await。  以前写 Javascript 的时候容易会遇到 callback hell，似乎 Promise 就是出来解决这个问题的，让你可以用同步的方式写异步程序。例如你有三个异步请求可以同时发出去，而后面的结果又需要这三个的结果都回来才能继续，那就可以用类似下面的伪代码，整体执行时间是最长的那个。 1 2 3  res1 = await test1 res2 = await test2 console.log(res1, res2)      Python 里面似乎也类似。我目前理解主要就是让程序可以「同步」执行，但是又避免了需要维护锁的问题，没有锁就不会有死锁了吧。。。  解释下同步，主要是针对对于 cpu 资源的占用。对于计算型的程序，实际上每时每刻都在利用 cpu 做计算，这样就算把计算拆分成了多个计算程序，让他们同时运行，那同一时刻还是只有一个程序在利用 cpu 资源执行，这样并行实际并不能提升效率。所以对于纯计算型任务，可以通过多进程利用多个 cpu。  但是实际我们的程序执行的时候，并不全是 cpu 计算，有时候会需要等网络 io，文件 io 等，做这些事情的时候实际上 cpu 是空闲的。协程就是让这些程序在等待的时候，把控制权交出来，让其他程序运行。那个 yield 关键字就是做这个事情的， yield 很像 return ，遇到的时候就会返回，暂停程序的执行，等到适当的时候又可以从暂停的地方继续执行。  以前是使用 @asyncio.coroutine 和 yield from 来创建协程，似乎 3.</description>
    </item>
    
  </channel>
</rss>