<!doctype html><html lang=en><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://wdicc.com/favicon.ico><title>hive 里面的 UDTF | wd and cc</title><meta name=title content="hive 里面的 UDTF"><meta name=description content='hive 支持 UDF， UDAF， UDTF，这几个让你使用 hive 更加便捷。

UDF


udf 就是一个自定义的 function，输入一个或多个参数，返回一个返回值，类似 substr/trim 之类。写起来比较简单，重构 UDF 类的 evaluate 方法就可以了。可以参考 http://richiehu.blog.51cto.com/2093113/386112 。

这是一个 urldecode 函数。


import org.apache.hadoop.hive.ql.exec.UDF;
import java.net.URLDecoder;

public final class urldecode extends UDF {

    public String evaluate(final String s) {
        if (s == null) { return null; }
        return getString(s);
    }

    public static String getString(String s) {
        String a;
        try {
            a = URLDecoder.decode(s);
        } catch ( Exception e) {
            a = "";
        }
        return a;
    }

    public static void main(String args[]) {
        String t = "%E5%A4%AA%E5%8E%9F-%E4%B8%89%E4%BA%9A";
        System.out.println( getString(t) );
    }
}

UDAF


udaf 就是自定义的聚合函数，类似 sum/avg 这类，输入多行的一个或多个参数，返回一个返回值。这个还没写过，可以参考 http://richiehu.blog.51cto.com/2093113/386113 。

&nbsp;


&nbsp;



UDTF


udtf 是针对输入一行，输出多行的需求来的，类似 explode 函数。可以参考 http://www.linezing.com/blog/?p=323 。

这个是输入数组字段，输出两列，一列是数组元素的位置，一列是数组元素。比 explode 多了一列位置，不过数组元素只能是 String 类型的。


import java.util.ArrayList;
import java.util.List;

//import org.apache.hadoop.io.Text;

import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.description;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;

import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

@description(
        name = "explodeWithPos",
        value = "_FUNC_(a) - separates the elements of array a into multiple rows with pos as first col "
        )

public class explodeWithPos extends GenericUDTF {

    ListObjectInspector listOI = null;

    @Override
        public void close() throws HiveException{
        }

    @Override
        public StructObjectInspector initialize(ObjectInspector [] args) throws UDFArgumentException {

            if (args.length != 1) {
                throw new UDFArgumentException("explodeWithPos() takes only one argument");
            }

            if (args[0].getCategory() != ObjectInspector.Category.LIST) {
                throw new UDFArgumentException("explodeWithPos() takes an array as a parameter");
            }

            listOI = (ListObjectInspector)args[0];

            ArrayList fieldNames = new ArrayList();
            ArrayList fieldOIs = new ArrayList();

            fieldNames.add("col1");
            //fieldOIs.add(listOI.getListElementObjectInspector());
            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

            fieldNames.add("col2");
            //fieldOIs.add(listOI.getListElementObjectInspector());
            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

            return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
        }

        //Object forwardObj[] = new Object[2];
        Object forwardObj[] = new String[2];

    @Override
        public void process(Object [] o) throws HiveException {

            List list = listOI.getList(o[0]);

            if ( list == null ) {
                return;
            }

            int i =0;
            for (Object r : list) {
                forwardObj[0] = String.valueOf(i);
                forwardObj[1] = r.toString();
                this.forward(forwardObj);
                i++;
            }
        }

    @Override
        public String toString() {
            return "explodeWithPos";
        }
}



'><meta name=keywords content="hive,"><meta property="og:url" content="https://wdicc.com/udf-in-hive/"><meta property="og:site_name" content="wd and cc"><meta property="og:title" content="hive 里面的 UDTF"><meta property="og:description" content='hive 支持 UDF， UDAF， UDTF，这几个让你使用 hive 更加便捷。
UDF
udf 就是一个自定义的 function，输入一个或多个参数，返回一个返回值，类似 substr/trim 之类。写起来比较简单，重构 UDF 类的 evaluate 方法就可以了。可以参考 http://richiehu.blog.51cto.com/2093113/386112 。
这是一个 urldecode 函数。
import org.apache.hadoop.hive.ql.exec.UDF; import java.net.URLDecoder; public final class urldecode extends UDF { public String evaluate(final String s) { if (s == null) { return null; } return getString(s); } public static String getString(String s) { String a; try { a = URLDecoder.decode(s); } catch ( Exception e) { a = ""; } return a; } public static void main(String args[]) { String t = "%E5%A4%AA%E5%8E%9F-%E4%B8%89%E4%BA%9A"; System.out.println( getString(t) ); } } UDAF udaf 就是自定义的聚合函数，类似 sum/avg 这类，输入多行的一个或多个参数，返回一个返回值。这个还没写过，可以参考 http://richiehu.blog.51cto.com/2093113/386113 。     UDTF udtf 是针对输入一行，输出多行的需求来的，类似 explode 函数。可以参考 http://www.linezing.com/blog/?p=323 。 这个是输入数组字段，输出两列，一列是数组元素的位置，一列是数组元素。比 explode 多了一列位置，不过数组元素只能是 String 类型的。 import java.util.ArrayList; import java.util.List; //import org.apache.hadoop.io.Text; import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.exec.description; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory; import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory; @description( name = "explodeWithPos", value = "_FUNC_(a) - separates the elements of array a into multiple rows with pos as first col " ) public class explodeWithPos extends GenericUDTF { ListObjectInspector listOI = null; @Override public void close() throws HiveException{ } @Override public StructObjectInspector initialize(ObjectInspector [] args) throws UDFArgumentException { if (args.length != 1) { throw new UDFArgumentException("explodeWithPos() takes only one argument"); } if (args[0].getCategory() != ObjectInspector.Category.LIST) { throw new UDFArgumentException("explodeWithPos() takes an array as a parameter"); } listOI = (ListObjectInspector)args[0]; ArrayList fieldNames = new ArrayList(); ArrayList fieldOIs = new ArrayList(); fieldNames.add("col1"); //fieldOIs.add(listOI.getListElementObjectInspector()); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldNames.add("col2"); //fieldOIs.add(listOI.getListElementObjectInspector()); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); } //Object forwardObj[] = new Object[2]; Object forwardObj[] = new String[2]; @Override public void process(Object [] o) throws HiveException { List list = listOI.getList(o[0]); if ( list == null ) { return; } int i =0; for (Object r : list) { forwardObj[0] = String.valueOf(i); forwardObj[1] = r.toString(); this.forward(forwardObj); i++; } } @Override public String toString() { return "explodeWithPos"; } }'><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2011-04-24T00:00:00+00:00"><meta property="article:modified_time" content="2011-04-24T00:00:00+00:00"><meta property="article:tag" content="Hive"><meta name=twitter:card content="summary"><meta name=twitter:title content="hive 里面的 UDTF"><meta name=twitter:description content='hive 支持 UDF， UDAF， UDTF，这几个让你使用 hive 更加便捷。
UDF
udf 就是一个自定义的 function，输入一个或多个参数，返回一个返回值，类似 substr/trim 之类。写起来比较简单，重构 UDF 类的 evaluate 方法就可以了。可以参考 http://richiehu.blog.51cto.com/2093113/386112 。
这是一个 urldecode 函数。
import org.apache.hadoop.hive.ql.exec.UDF; import java.net.URLDecoder; public final class urldecode extends UDF { public String evaluate(final String s) { if (s == null) { return null; } return getString(s); } public static String getString(String s) { String a; try { a = URLDecoder.decode(s); } catch ( Exception e) { a = ""; } return a; } public static void main(String args[]) { String t = "%E5%A4%AA%E5%8E%9F-%E4%B8%89%E4%BA%9A"; System.out.println( getString(t) ); } } UDAF udaf 就是自定义的聚合函数，类似 sum/avg 这类，输入多行的一个或多个参数，返回一个返回值。这个还没写过，可以参考 http://richiehu.blog.51cto.com/2093113/386113 。     UDTF udtf 是针对输入一行，输出多行的需求来的，类似 explode 函数。可以参考 http://www.linezing.com/blog/?p=323 。 这个是输入数组字段，输出两列，一列是数组元素的位置，一列是数组元素。比 explode 多了一列位置，不过数组元素只能是 String 类型的。 import java.util.ArrayList; import java.util.List; //import org.apache.hadoop.io.Text; import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.exec.description; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory; import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory; @description( name = "explodeWithPos", value = "_FUNC_(a) - separates the elements of array a into multiple rows with pos as first col " ) public class explodeWithPos extends GenericUDTF { ListObjectInspector listOI = null; @Override public void close() throws HiveException{ } @Override public StructObjectInspector initialize(ObjectInspector [] args) throws UDFArgumentException { if (args.length != 1) { throw new UDFArgumentException("explodeWithPos() takes only one argument"); } if (args[0].getCategory() != ObjectInspector.Category.LIST) { throw new UDFArgumentException("explodeWithPos() takes an array as a parameter"); } listOI = (ListObjectInspector)args[0]; ArrayList fieldNames = new ArrayList(); ArrayList fieldOIs = new ArrayList(); fieldNames.add("col1"); //fieldOIs.add(listOI.getListElementObjectInspector()); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldNames.add("col2"); //fieldOIs.add(listOI.getListElementObjectInspector()); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); } //Object forwardObj[] = new Object[2]; Object forwardObj[] = new String[2]; @Override public void process(Object [] o) throws HiveException { List list = listOI.getList(o[0]); if ( list == null ) { return; } int i =0; for (Object r : list) { forwardObj[0] = String.valueOf(i); forwardObj[1] = r.toString(); this.forward(forwardObj); i++; } } @Override public String toString() { return "explodeWithPos"; } }'><meta itemprop=name content="hive 里面的 UDTF"><meta itemprop=description content='hive 支持 UDF， UDAF， UDTF，这几个让你使用 hive 更加便捷。
UDF
udf 就是一个自定义的 function，输入一个或多个参数，返回一个返回值，类似 substr/trim 之类。写起来比较简单，重构 UDF 类的 evaluate 方法就可以了。可以参考 http://richiehu.blog.51cto.com/2093113/386112 。
这是一个 urldecode 函数。
import org.apache.hadoop.hive.ql.exec.UDF; import java.net.URLDecoder; public final class urldecode extends UDF { public String evaluate(final String s) { if (s == null) { return null; } return getString(s); } public static String getString(String s) { String a; try { a = URLDecoder.decode(s); } catch ( Exception e) { a = ""; } return a; } public static void main(String args[]) { String t = "%E5%A4%AA%E5%8E%9F-%E4%B8%89%E4%BA%9A"; System.out.println( getString(t) ); } } UDAF udaf 就是自定义的聚合函数，类似 sum/avg 这类，输入多行的一个或多个参数，返回一个返回值。这个还没写过，可以参考 http://richiehu.blog.51cto.com/2093113/386113 。     UDTF udtf 是针对输入一行，输出多行的需求来的，类似 explode 函数。可以参考 http://www.linezing.com/blog/?p=323 。 这个是输入数组字段，输出两列，一列是数组元素的位置，一列是数组元素。比 explode 多了一列位置，不过数组元素只能是 String 类型的。 import java.util.ArrayList; import java.util.List; //import org.apache.hadoop.io.Text; import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.exec.description; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory; import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory; @description( name = "explodeWithPos", value = "_FUNC_(a) - separates the elements of array a into multiple rows with pos as first col " ) public class explodeWithPos extends GenericUDTF { ListObjectInspector listOI = null; @Override public void close() throws HiveException{ } @Override public StructObjectInspector initialize(ObjectInspector [] args) throws UDFArgumentException { if (args.length != 1) { throw new UDFArgumentException("explodeWithPos() takes only one argument"); } if (args[0].getCategory() != ObjectInspector.Category.LIST) { throw new UDFArgumentException("explodeWithPos() takes an array as a parameter"); } listOI = (ListObjectInspector)args[0]; ArrayList fieldNames = new ArrayList(); ArrayList fieldOIs = new ArrayList(); fieldNames.add("col1"); //fieldOIs.add(listOI.getListElementObjectInspector()); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldNames.add("col2"); //fieldOIs.add(listOI.getListElementObjectInspector()); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); } //Object forwardObj[] = new Object[2]; Object forwardObj[] = new String[2]; @Override public void process(Object [] o) throws HiveException { List list = listOI.getList(o[0]); if ( list == null ) { return; } int i =0; for (Object r : list) { forwardObj[0] = String.valueOf(i); forwardObj[1] = r.toString(); this.forward(forwardObj); i++; } } @Override public String toString() { return "explodeWithPos"; } }'><meta itemprop=datePublished content="2011-04-24T00:00:00+00:00"><meta itemprop=dateModified content="2011-04-24T00:00:00+00:00"><meta itemprop=wordCount content="506"><meta itemprop=keywords content="Hive"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:720px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#01242e;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#000;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}time{font-family:monospace;font-style:normal;font-size:15px}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto !important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h2>wd and cc</h2></a><p>-- Good good study, day day up!</p><nav><a href=/>Home</a>
<a href=/posts/>Posts</a>
<a href="https://www.google.com.hk/search?sitesearch=https%3A%2F%2Fwdicc.com%2F&amp;q=">Search</a>
<a href=/tags/>Tags</a>
<a href=/atom.xml>subscribe</a></nav></header><main><content>hive 支持 UDF， UDAF， UDTF，这几个让你使用 hive 更加便捷。<br><div id=outline-container-1 class=outline-2><br><h2 id=sec-1>UDF</h2><br><div id=text-1 class=outline-text-2><br>udf 就是一个自定义的 function，输入一个或多个参数，返回一个返回值，类似 substr/trim 之类。写起来比较简单，重构 UDF 类的 evaluate 方法就可以了。可以参考 <a href=http://richiehu.blog.51cto.com/2093113/386112>http://richiehu.blog.51cto.com/2093113/386112</a> 。<br>这是一个 urldecode 函数。<br><pre class="prettyprint lang-java">

import org.apache.hadoop.hive.ql.exec.UDF;
import java.net.URLDecoder;

public final class urldecode extends UDF {

    public String evaluate(final String s) {
        if (s == null) { return null; }
        return getString(s);
    }

    public static String getString(String s) {
        String a;
        try {
            a = URLDecoder.decode(s);
        } catch ( Exception e) {
            a = "";
        }
        return a;
    }

    public static void main(String args[]) {
        String t = "%E5%A4%AA%E5%8E%9F-%E4%B8%89%E4%BA%9A";
        System.out.println( getString(t) );
    }
}</pre><div id=outline-container-2 class=outline-2><h2 id=sec-2>UDAF</h2><div id=text-2 class=outline-text-2>udaf 就是自定义的聚合函数，类似 sum/avg 这类，输入多行的一个或多个参数，返回一个返回值。这个还没写过，可以参考 <a href=http://richiehu.blog.51cto.com/2093113/386113>http://richiehu.blog.51cto.com/2093113/386113</a> 。
&nbsp;</div>&nbsp;</div><div id=outline-container-3 class=outline-2><h2 id=sec-3>UDTF</h2><div id=text-3 class=outline-text-2>udtf 是针对输入一行，输出多行的需求来的，类似 explode 函数。可以参考 <a href="http://www.linezing.com/blog/?p=323">http://www.linezing.com/blog/?p=323</a> 。
这个是输入数组字段，输出两列，一列是数组元素的位置，一列是数组元素。比 explode 多了一列位置，不过数组元素只能是 String 类型的。<pre class="prettyprint lang-java">

import java.util.ArrayList;
import java.util.List;

//import org.apache.hadoop.io.Text;

import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.description;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;

import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

@description(
        name = "explodeWithPos",
        value = "_FUNC_(a) - separates the elements of array a into multiple rows with pos as first col "
        )

public class explodeWithPos extends GenericUDTF {

    ListObjectInspector listOI = null;

    @Override
        public void close() throws HiveException{
        }

    @Override
        public StructObjectInspector initialize(ObjectInspector [] args) throws UDFArgumentException {

            if (args.length != 1) {
                throw new UDFArgumentException("explodeWithPos() takes only one argument");
            }

            if (args[0].getCategory() != ObjectInspector.Category.LIST) {
                throw new UDFArgumentException("explodeWithPos() takes an array as a parameter");
            }

            listOI = (ListObjectInspector)args[0];

            ArrayList fieldNames = new ArrayList();
            ArrayList fieldOIs = new ArrayList();

            fieldNames.add("col1");
            //fieldOIs.add(listOI.getListElementObjectInspector());
            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

            fieldNames.add("col2");
            //fieldOIs.add(listOI.getListElementObjectInspector());
            fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

            return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
        }

        //Object forwardObj[] = new Object[2];
        Object forwardObj[] = new String[2];

    @Override
        public void process(Object [] o) throws HiveException {

            List list = listOI.getList(o[0]);

            if ( list == null ) {
                return;
            }

            int i =0;
            for (Object r : list) {
                forwardObj[0] = String.valueOf(i);
                forwardObj[1] = r.toString();
                this.forward(forwardObj);
                i++;
            }
        }

    @Override
        public String toString() {
            return "explodeWithPos";
        }
}</pre></div></div></div></div></content><div><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//wdicc.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>